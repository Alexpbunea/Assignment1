{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11197806,"sourceType":"datasetVersion","datasetId":6991235},{"sourceId":11198748,"sourceType":"datasetVersion","datasetId":6991926},{"sourceId":11198770,"sourceType":"datasetVersion","datasetId":6991945},{"sourceId":11198931,"sourceType":"datasetVersion","datasetId":6992066}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install compressed-tensors #ignore this, I tried to install a quantized model of mistral but nothing but errors were thrown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T15:51:24.905343Z","iopub.execute_input":"2025-03-28T15:51:24.905608Z","iopub.status.idle":"2025-03-28T15:51:30.519045Z","shell.execute_reply.started":"2025-03-28T15:51:24.905586Z","shell.execute_reply":"2025-03-28T15:51:30.517962Z"}},"outputs":[{"name":"stdout","text":"Collecting compressed-tensors\n  Downloading compressed_tensors-0.9.2-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from compressed-tensors) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from compressed-tensors) (4.47.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from compressed-tensors) (2.11.0a2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->compressed-tensors) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->compressed-tensors) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->compressed-tensors) (4.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->compressed-tensors) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->compressed-tensors) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->compressed-tensors) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->compressed-tensors) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->compressed-tensors) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->compressed-tensors) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->compressed-tensors) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->compressed-tensors) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->compressed-tensors) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->compressed-tensors) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->compressed-tensors) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->compressed-tensors) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->compressed-tensors) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->compressed-tensors) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->compressed-tensors) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->compressed-tensors) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->compressed-tensors) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->compressed-tensors) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->compressed-tensors) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->compressed-tensors) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->compressed-tensors) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->compressed-tensors) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->compressed-tensors) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->compressed-tensors) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->compressed-tensors) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->compressed-tensors) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers->compressed-tensors) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers->compressed-tensors) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers->compressed-tensors) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers->compressed-tensors) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers->compressed-tensors) (2024.2.0)\nDownloading compressed_tensors-0.9.2-py3-none-any.whl (97 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: compressed-tensors\nSuccessfully installed compressed-tensors-0.9.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:04:38.265524Z","iopub.execute_input":"2025-03-28T16:04:38.265750Z","iopub.status.idle":"2025-03-28T16:04:43.420842Z","shell.execute_reply.started":"2025-03-28T16:04:38.265728Z","shell.execute_reply":"2025-03-28T16:04:43.419643Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import Dataset\nimport json\nimport torch\n\nfrom huggingface_hub import login\n\nlogin(token=\"put your token, if necessary here\")\n\n\nprint(\"CUDA disponible:\", torch.cuda.is_available())\ntorch.cuda.empty_cache()\n\n\nmodel_name = \"google-t5/t5-base\"  \ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n\ndataset_path = \"/kaggle/input/input-simple/train_dataset.jsonl\"\ntrain_data = Dataset.from_json(dataset_path)\n\ndef tokenize_function(batch):\n\n    inputs = batch[\"input\"]\n    inputs = [(\" \".join(i) if isinstance(i, list) else i).strip() for i in inputs]\n    inputs = [i if i else \"No input provided.\" for i in inputs]\n\n    outputs = batch[\"output\"]\n    outputs = [(\", \".join(str(o) for o in i) if isinstance(i, list) else i).strip() for i in outputs]\n    outputs = [o if o else \"No output provided.\" for o in outputs]\n\n    input_encodings = tokenizer(inputs, truncation=True, max_length=512, padding=\"max_length\")\n    output_encodings = tokenizer(outputs, truncation=True, max_length=64, padding=\"max_length\")\n\n    labels = [\n        [(label if label != tokenizer.pad_token_id else -100) for label in output]\n        for output in output_encodings[\"input_ids\"]\n    ]\n\n    return {\n        \"input_ids\": input_encodings[\"input_ids\"],\n        \"attention_mask\": input_encodings[\"attention_mask\"],\n        \"labels\": labels,\n    }\n\ntokenized_train = train_data.map(tokenize_function, batched=True, num_proc=4)\n\ndata_split = tokenized_train.train_test_split(test_size=0.20)\ntrain_dataset = data_split[\"train\"]\neval_dataset = data_split[\"test\"]\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/results2\",\n    eval_steps=200,  \n    save_steps=400,\n    learning_rate=4e-5,\n    evaluation_strategy=\"steps\",  \n    save_strategy=\"steps\", \n    per_device_train_batch_size=4,  \n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=2,\n    logging_dir=\"/kaggle/working/logs2\",\n    report_to=[],  \n    logging_steps=100,\n    gradient_accumulation_steps=2, \n    lr_scheduler_type=\"cosine\",\n    load_best_model_at_end=True,  \n    metric_for_best_model=\"eval_loss\",  \n    greater_is_better=False,  \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\n\ntrainer.train()\n\n\nmodel.save_pretrained(\"/kaggle/working/fine_tuned_t5_base\")\ntokenizer.save_pretrained(\"/kaggle/working/fine_tuned_t5_base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:17:54.768744Z","iopub.execute_input":"2025-03-29T11:17:54.769110Z","iopub.status.idle":"2025-03-29T12:01:00.657642Z","shell.execute_reply.started":"2025-03-29T11:17:54.769082Z","shell.execute_reply":"2025-03-29T12:01:00.656591Z"}},"outputs":[{"name":"stdout","text":"CUDA disponible: True\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-3-25be3dece72b>:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2829' max='2829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2829/2829 42:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.105500</td>\n      <td>0.802409</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.859900</td>\n      <td>0.673343</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.747800</td>\n      <td>0.591101</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.688900</td>\n      <td>0.527997</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.611300</td>\n      <td>0.498179</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.559500</td>\n      <td>0.455288</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.523200</td>\n      <td>0.436658</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.516900</td>\n      <td>0.417634</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.482900</td>\n      <td>0.405880</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.424500</td>\n      <td>0.393323</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.432600</td>\n      <td>0.388785</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.425000</td>\n      <td>0.387302</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.428400</td>\n      <td>0.385409</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.453700</td>\n      <td>0.386654</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Could not locate the best model at /kaggle/working/results2/checkpoint-2600/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/fine_tuned_t5_base/tokenizer_config.json',\n '/kaggle/working/fine_tuned_t5_base/special_tokens_map.json',\n '/kaggle/working/fine_tuned_t5_base/spiece.model',\n '/kaggle/working/fine_tuned_t5_base/added_tokens.json')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nimport json\nimport torch\n\n\ndef generate_outputs(input_data):\n    model = T5ForConditionalGeneration.from_pretrained(\"/kaggle/working/fine_tuned_t5_base2\")\n    tokenizer = T5Tokenizer.from_pretrained(\"/kaggle/working/fine_tuned_t5_base2\")\n\n    inputs = [example[\"input\"] for example in input_data] \n    batch_size = 4\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n\n    decoded_outputs = []\n\n    for i in range(0, len(inputs), batch_size):\n        batch_inputs = inputs[i:i + batch_size]\n        inputs_encodings = tokenizer(batch_inputs, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n        inputs_encodings = {key: val.to(device) for key, val in inputs_encodings.items()}\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=inputs_encodings[\"input_ids\"],\n                attention_mask=inputs_encodings[\"attention_mask\"],\n                max_length=64, \n                num_beams=5,  \n                early_stopping=True\n            )\n        batch_decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n        decoded_outputs.extend(batch_decoded_outputs)\n\n    output_file_path = \"/kaggle/working/generated_outputs.jsonl\"\n    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n        for input_example, output in zip(input_data, decoded_outputs):\n            f.write(json.dumps({\"input\": input_example[\"input\"], \"output\": output}) + \"\\n\")\n\ndef load_json(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return json.load(f)\n\npath = load_json(\"/kaggle/input/groundtruth/dev_dataset.jsonl\")\ngenerate_outputs(path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}